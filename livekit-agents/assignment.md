# ğŸ™ï¸ LiveKit Intelligent Interruption Handling Challenge
<!-- pip install -e ".[deepgram,elevenlabs,openai]" -->
### **AI Agent Conversational Flow Enhancement**

---

## ğŸ“Œ **Repository Instructions**

**Fork and work on this repository:**
ğŸ‘‰ [https://github.com/Dark-Sys-Jenkins/agents-assignment](https://github.com/Dark-Sys-Jenkins/agents-assignment)

â— **DO NOT raise a PR in the original LiveKit repo.**
All work must remain inside your fork.

---

## ğŸ“ **Overview**

This challenge tests your ability to refine the conversational flow of a **real-time AI agent**.

### â— The Problem

LiveKitâ€™s default Voice Activity Detection (VAD) is **too sensitive**.
When the agent is speaking and the user says short acknowledgements like:

* "yeah"
* "ok"
* "aha"
* "hmm"

â€¦ the agent incorrectly interprets them as **interruptions**, causing it to stop mid-sentence.

These words are called **backchanneling**, and they should *not* break the agentâ€™s speech.

---

## ğŸ¯ **Goal**

You must implement a **context-aware logic layer** that distinguishes between:

* **Passive acknowledgements** â†’ should *not* interrupt the agent
* **Active interruptions** â†’ should *instantly* stop the agent
* **Normal responses** â†’ should be processed normally when the agent is silent

---

## ğŸ§  **Core Logic Matrix**

| User Input Type    | Agent State    | Desired Behavior                                  |
| ------------------ | -------------- | ------------------------------------------------- |
| â€œYeah / Ok / Hmmâ€  | Agent Speaking | **IGNORE** â†’ Agent continues speaking smoothly    |
| â€œWait / Stop / Noâ€ | Agent Speaking | **INTERRUPT** â†’ Agent stops immediately           |
| â€œYeah / Ok / Hmmâ€  | Agent Silent   | **RESPOND** â†’ Treat as valid conversational input |
| â€œStart / Helloâ€    | Agent Silent   | **RESPOND** â†’ Normal conversational reply         |

---

## ğŸ”‘ **Key Features to Implement**

### 1ï¸âƒ£ **Configurable Ignore List**

A list of â€œsoftâ€ acknowledgement words such as:
`['yeah', 'ok', 'hmm', 'right', 'uh-huh']`

These words should be ignored **only when the agent is speaking**.

---

### 2ï¸âƒ£ **State-Based Filtering**

Your logic must check:

* Is the agent currently **speaking / playing audio**?
  â†’ Ignore filler words.

* Is the agent **silent**?
  â†’ Process them like normal responses.

---

### 3ï¸âƒ£ **Semantic Interruption Handling**

If the user says a mixed input like:

> â€œYeah wait a second.â€

This contains an interruption word ("wait"), so the agent must **stop immediately**.

---

### 4ï¸âƒ£ **No VAD Modification**

âš ï¸ Do **not** modify the lower-level VAD kernel.

You must implement this logic **within the agentâ€™s event loop**, filtering input intelligently.

---

## âš™ï¸ **Technical Expectations**

### ğŸ§© Integration

Work within the existing **LiveKit Agent framework** in the given repo.

### ğŸ—£ï¸ Transcription Logic

You will rely on **Speech-to-Text (STT)**.
Because **VAD triggers earlier than STT**, you must design a strategy to avoid:

* false interruptions
* premature stopping
* audio stutters

Hint:
You may need to **queue interruptions** and validate STT text before finalizing the interruption.

### âš¡ Real-Time Constraint

Your solution must remain **strictly real-time**.
Any delay introduced must be **imperceptible** to the user.

---

## ğŸ§ª **Test Scenarios**

### âœ… **Scenario 1: The Long Explanation**

* Agent: speaking a long paragraph
* User: â€œOkayâ€¦ yeahâ€¦ uh-huhâ€
* âœ”ï¸ Result: Agent continues uninterrupted

---

### âœ… **Scenario 2: Passive Affirmation**

* Agent: â€œAre you ready?â€ (silent afterward)
* User: â€œYeah.â€
* âœ”ï¸ Result: Agent processes and continues naturally
  â†’ â€œOkay, starting now.â€

---

### âœ… **Scenario 3: The Correction**

* Agent: â€œOne, two, threeâ€¦â€
* User: â€œNo stop.â€
* âœ”ï¸ Result: Agent cuts off immediately

---

### âœ… **Scenario 4: Mixed Input**

* Agent: speaking
* User: â€œYeah okay but wait.â€
* âœ”ï¸ Result: Agent stops (because â€œwaitâ€ is an interruption word)

---

## ğŸ“‚ **Outcome**

By completing this challenge, you will demonstrate your ability to build **robust conversational handling**, making the agent:

* More natural
* Less sensitive to backchanneling
* Fully interruption-aware
* Real-time stable

---
